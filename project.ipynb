{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b43765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model.lenet import load_lenet\n",
    "from dataset.dataset import load_data, create_dataloaders, visualize_5_samples\n",
    "from functions.optimizer import load_optimizer\n",
    "from functions.loss import load_loss_fun\n",
    "from functions.functions import train_model, eval_model, save_checkpoint, load_checkpoint, visualize_5_sample_dynamics\n",
    "from functions.xai import explain_dataset, visualize_explanation, evaluate_explainations, visualize_k_expl\n",
    "from functions.xil import xil_loop, compute_simplicity, random_sampling, simplicity_sampling\n",
    "from utils.utils import enable_reproducibility\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup reproducibility and device\n",
    "SEED=123\n",
    "enable_reproducibility(SEED)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "STEP1 = True\n",
    "STEP2 = True\n",
    "STEP3 = True\n",
    "STEP4 = True\n",
    "STEP5 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(dataset: str = \"DecoyMNIST\"):\n",
    "  model = load_lenet(device)\n",
    "  optim = load_optimizer(\"SGD\", model.parameters(), lr=1.0e-2, weight_decay=0)\n",
    "  loss = load_loss_fun(\"CrossEntropy\")\n",
    "  train_set, val_set, test_set = load_data(\n",
    "    dataset, \n",
    "    seed=SEED, \n",
    "    reload=True,\n",
    "    bias_ratio=[1]*10\n",
    "  )\n",
    "  \n",
    "  data = [train_set, val_set, test_set]\n",
    "  params = {\"batch_size\":32}\n",
    "  m_params = [params]*3\n",
    "  train_loader, val_loader, test_loader = create_dataloaders(data, m_params)\n",
    "\n",
    "  _, _ = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optim, \n",
    "    loss, \n",
    "    n_epochs=10, \n",
    "    eval_loader=val_loader, \n",
    "    device=device\n",
    "  )\n",
    "  loss, acc = eval_model(model, test_loader, loss,  device)\n",
    "  print(\"=\"*20,f\"Test set Loss:{loss:.2f} | Acc:{acc:.2f}.\",\"=\"*20)\n",
    "\n",
    "  all_attr, all_imgs = explain_dataset(train_loader, model, device)\n",
    "  exp_err = evaluate_explainations(all_attr, torch.from_numpy(train_set.masks))\n",
    "  print(\"=\"*20,f\"Train explaination error {exp_err:.2}\",\"=\"*20)\n",
    "\n",
    "  return train_set, test_set, all_attr, all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ebd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEP1:\n",
    "  train_set, test_set, all_attr, all_imgs = step1(\"DecoyMNIST\")\n",
    "  visualize_5_samples(train_set, 0)\n",
    "  visualize_k_expl(all_attr, all_imgs, train_set, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(dataset: str = \"DecoyMNIST\"):\n",
    "  model = load_lenet(device)\n",
    "  optim = load_optimizer(\"SGD\", model.parameters(), lr=1.0e-2, weight_decay=0)\n",
    "  rrr_reg = 1 if dataset == \"DecoyMNIST\" else 1e-2\n",
    "  loss = load_loss_fun(\"RRR\", reg_rate=rrr_reg)\n",
    "  train_set, val_set, test_set = load_data(\n",
    "    dataset, \n",
    "    seed=SEED, \n",
    "    reload=True,\n",
    "    bias_ratio=[1]*10\n",
    "  )\n",
    "  \n",
    "  data = [train_set, val_set, test_set]\n",
    "  params = {\"batch_size\":32}\n",
    "  m_params = [params]*3\n",
    "  train_loader, val_loader, test_loader = create_dataloaders(data, m_params)\n",
    "\n",
    "  _, _ = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optim, \n",
    "    loss, \n",
    "    n_epochs=10, \n",
    "    eval_loader=val_loader, \n",
    "    device=device\n",
    "  )\n",
    "  ce_loss = load_loss_fun(\"CrossEntropy\")\n",
    "  loss, acc = eval_model(model, test_loader, ce_loss,  device)\n",
    "  print(\"=\"*20,f\"Test set Loss:{loss:.2f} | Acc:{acc:.2f}.\",\"=\"*20)\n",
    "\n",
    "  all_attr, all_imgs = explain_dataset(train_loader, model, device)\n",
    "  exp_err = evaluate_explainations(all_attr, torch.from_numpy(train_set.masks))\n",
    "  print(\"=\"*20,f\"Train explaination error {exp_err:.2}\",\"=\"*20)\n",
    "\n",
    "  return train_set, test_set, all_attr, all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEP2:\n",
    "  train_set, test_set, all_attr, all_imgs = step2(\"DecoyFashionMNIST\")\n",
    "  visualize_5_samples(train_set, 0)\n",
    "  visualize_k_expl(all_attr, all_imgs, train_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(dataset: str = \"DecoyMNIST\"):\n",
    "  model = load_lenet(device)\n",
    "  train_set, val_set, test_set = load_data(\n",
    "    dataset, \n",
    "    seed=SEED, \n",
    "    reload=True,\n",
    "    bias_ratio=[1]*10\n",
    "  )\n",
    "  \n",
    "  data = [train_set, val_set, test_set]\n",
    "  params = {\"batch_size\":32}\n",
    "  m_params = [params]*3\n",
    "  train_loader, val_loader, test_loader = create_dataloaders(data, m_params)\n",
    "\n",
    "  query = xil_loop(\n",
    "    train_set,\n",
    "    model, \n",
    "    random_sampling,\n",
    "    300,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    step_size=100,\n",
    "    rrr_reg_rate=1 if dataset==\"DecoyMNIST\" else 1e-2,\n",
    "    device=device\n",
    "  )\n",
    "  print(f\"It took {query} iterations.\")\n",
    "  return train_set, test_set, all_attr, all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEP3:\n",
    "  step3(\"DecoyMNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(dataset: str = \"DecoyMNIST\", metric: str= \"MP\"):\n",
    "  model = load_lenet(device)\n",
    "  optim = load_optimizer(\"SGD\", model.parameters(), lr=1.0e-2, weight_decay=0)\n",
    "  loss = load_loss_fun(\"CrossEntropy\")\n",
    "  train_set, val_set, test_set = load_data(\n",
    "    dataset, \n",
    "    seed=SEED, \n",
    "    reload=True,\n",
    "    bias_ratio=[0.95]*10\n",
    "  )\n",
    "  \n",
    "  data = [train_set, val_set, test_set]\n",
    "  params = {\"batch_size\":32}\n",
    "  m_params = [params]*3\n",
    "  train_loader, val_loader, test_loader = create_dataloaders(data, m_params)\n",
    "\n",
    "  _, dyn = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optim, \n",
    "    loss, \n",
    "    n_epochs=10, \n",
    "    eval_loader=val_loader, \n",
    "    device=device\n",
    "  )\n",
    "  loss, acc = eval_model(model, test_loader, loss,  device)\n",
    "  print(\"=\"*20,f\"Test set Loss:{loss:.2f} | Acc:{acc:.2f}.\",\"=\"*20)\n",
    "\n",
    "  all_attr, all_imgs = explain_dataset(train_loader, model, device)\n",
    "  exp_err = evaluate_explainations(all_attr, torch.from_numpy(train_set.masks))\n",
    "  print(\"=\"*20,f\"Train explaination error {exp_err:.2}\",\"=\"*20)\n",
    "  \n",
    "  simplicity = compute_simplicity(dyn, metric=metric)\n",
    "  scores = []\n",
    "  is_confounded = []\n",
    "  class_indeces = range(len(train_set))\n",
    "  for id in class_indeces:\n",
    "    index, _,_,mask = train_set[id]\n",
    "    scores.append(simplicity[index])\n",
    "    if mask.sum() > 1:\n",
    "      is_confounded.append(1)\n",
    "    else:\n",
    "      is_confounded.append(0)\n",
    "  \n",
    "  print(f\"All samples Correlation:{pearsonr(scores, is_confounded)}\")\n",
    "\n",
    "\n",
    "  # Class-wise correlation\n",
    "  for label in range(10):\n",
    "    class_indeces = np.where(train_set.y == label)[0]\n",
    "    print(\"samples len\", len(class_indeces))\n",
    "    scores = []\n",
    "    is_confounded = []\n",
    "    for id in class_indeces:\n",
    "      index, _,_,mask = train_set[id]\n",
    "      scores.append(simplicity[index])\n",
    "      if mask.sum() > 1:\n",
    "        is_confounded.append(1)\n",
    "      else:\n",
    "        is_confounded.append(0)\n",
    "    print(f\"{label} Confounded samples:\", sum(1 for x in is_confounded if x == 0))\n",
    "\n",
    "    print(f\"{label} Correlation:{pearsonr(scores, is_confounded)}\")\n",
    "\n",
    "  return train_set, test_set, dyn, all_attr, all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd01731",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEP4:\n",
    "  train_set, test_set, dyn, all_attr, all_imgs = step4(\"DecoyFashionMNIST\", \"EC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5(dataset: str = \"DecoyMNIST\"):\n",
    "  model = load_lenet(device)\n",
    "  train_set, val_set, test_set = load_data(\n",
    "    dataset, \n",
    "    seed=SEED, \n",
    "    reload=True,\n",
    "    bias_ratio=[1]*10\n",
    "  )\n",
    "  \n",
    "  data = [train_set, val_set, test_set]\n",
    "  params = {\"batch_size\":32}\n",
    "  m_params = [params]*3\n",
    "  train_loader, val_loader, test_loader = create_dataloaders(data, m_params)\n",
    "\n",
    "  optim = load_optimizer(\"SGD\", model.parameters(), lr=1.0e-2, weight_decay=0)\n",
    "  loss = load_loss_fun(\"CrossEntropy\")\n",
    "  _, dynamics = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    optim, \n",
    "    loss, \n",
    "    n_epochs=10, \n",
    "    eval_loader=val_loader, \n",
    "    device=device\n",
    "  )\n",
    "\n",
    "  query = xil_loop(\n",
    "    train_set,\n",
    "    model, \n",
    "    simplicity_sampling,\n",
    "    2400,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    tr_dynamics=dynamics,\n",
    "    step_size=100,\n",
    "    rrr_reg_rate=1 if dataset==\"DecoyMNIST\" else 1e-2,\n",
    "    device=device\n",
    "  )\n",
    "  print(f\"It took {query} iterations.\")\n",
    "  return train_set, test_set, all_attr, all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEP5:\n",
    "  step5(\"DecoyMNIST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
